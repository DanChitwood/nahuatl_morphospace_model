{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc13260-6d50-4b35-a03e-0a7d3bd435df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for working with dataframes\n",
    "import os\n",
    "from os import listdir, makedirs # for retrieving files from directory\n",
    "from os.path import isfile, join # for retrieving files from directory\n",
    "from pathlib import Path # for retrieving files from directory\n",
    "import networkx as nx # for making graphs\n",
    "import numpy as np # for arrays\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib.image as mpimg # for images\n",
    "from scipy.interpolate import interp1d # for interpolating points\n",
    "from scipy.spatial import procrustes # for Procrustes analysis\n",
    "from sklearn.decomposition import PCA # for principal component analysis\n",
    "import seaborn as sns# ; sns.set(color_codes=True) # for plotting in seaborn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA\n",
    "from sklearn.metrics import confusion_matrix # for confusion matrix\n",
    "from sklearn.model_selection import StratifiedKFold # for Stratified K fold sampling\n",
    "from sklearn.cluster import AgglomerativeClustering # for agglomerative clustering\n",
    "import scipy.stats as stats # for kruskal wallis test\n",
    "import statsmodels.stats.multitest as multitest # multiple test adjustment\n",
    "from numpy.linalg import det # for sampling higher dimensional convex hull\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial import Delaunay\n",
    "import math\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.ops import nearest_points\n",
    "import trimesh\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7648c9d-7bd3-4c96-bb8c-f74aff77dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### FUNCTIONS ###\n",
    "#################\n",
    "\n",
    "def angle_between(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex\n",
    "    inputs: three angle points, as tuples\n",
    "    output: angle in degrees\n",
    "    \"\"\"\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360\n",
    "    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360\n",
    "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n",
    "\n",
    "def rotate_points(xvals, yvals, degrees):\n",
    "    \"\"\"\"\n",
    "    define a function to rotate 2D x and y coordinate points around the origin\n",
    "    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate\n",
    "    outputs: rotated and y vals\n",
    "    \"\"\"\n",
    "    angle_to_move = 90-degrees\n",
    "    rads = np.deg2rad(angle_to_move)\n",
    "    \n",
    "    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)\n",
    "    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)\n",
    "    \n",
    "    return new_xvals, new_yvals\n",
    "\n",
    "def interpolation(x, y, number): \n",
    "    \"\"\"\n",
    "    define a function to return equally spaced, interpolated points for a given polyline\n",
    "    inputs: arrays of x and y values for a polyline, number of points to interpolate\n",
    "    ouputs: interpolated points along the polyline, inclusive of start and end points\n",
    "    \"\"\"\n",
    "    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))\n",
    "    distance = distance/distance[-1]\n",
    "\n",
    "    fx, fy = interp1d( distance, x ), interp1d( distance, y )\n",
    "\n",
    "    alpha = np.linspace(0, 1, number)\n",
    "    x_regular, y_regular = fx(alpha), fy(alpha)\n",
    "    \n",
    "    return x_regular, y_regular\n",
    "\n",
    "def euclid_dist(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    define a function to return the euclidean distance between two points\n",
    "    inputs: x and y values of the two points\n",
    "    output: the eulidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "\n",
    "def poly_area(x,y):\n",
    "    \"\"\"\n",
    "    define a function to calculate the area of a polygon using the shoelace algorithm\n",
    "    inputs: separate numpy arrays of x and y coordinate values\n",
    "    outputs: the area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def gpa_mean(leaf_arr, landmark_num, dim_num):\n",
    "    \n",
    "    \"\"\"\n",
    "    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean\n",
    "    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions\n",
    "    output: an array of the Generalized Procrustes Analysis mean shape\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to\n",
    "    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape\n",
    "\n",
    "    mean_diff = 10**(-30) # set a distance between means to stop the algorithm\n",
    "\n",
    "    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape\n",
    "\n",
    "    d = 1000000 # set d initially arbitraily high\n",
    "\n",
    "    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations\n",
    "\n",
    "        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals\n",
    "\n",
    "        for i in range(len(leaf_arr)): # for each leaf shape \n",
    "\n",
    "            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf\n",
    "            arr[i] = s2 # store procrustes adjusted shape to array\n",
    "\n",
    "        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference\n",
    "\n",
    "        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean\n",
    "\n",
    "        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration\n",
    "\n",
    "    return new_mean\n",
    "\n",
    "# From: https://stackoverflow.com/questions/59073952/how-to-get-uniformly-distributed-points-in-convex-hull\n",
    "# Accessed 11 February 2024\n",
    "\n",
    "def dist_in_hull(points, n):\n",
    "    dims = points.shape[-1]\n",
    "    hull = points[ConvexHull(points).vertices]\n",
    "    deln = hull[Delaunay(hull).simplices]\n",
    "\n",
    "    vols = np.abs(det(deln[:, :dims, :] - deln[:, dims:, :])) / math.factorial(dims)    \n",
    "    sample = np.random.choice(len(vols), size = n, p = vols / vols.sum())\n",
    "\n",
    "    return np.einsum('ijk, ij -> ik', deln[sample], dirichlet.rvs([1]*(dims + 1), size = n))\n",
    "\n",
    "def is_self_intersecting(polygon_coordinates):\n",
    "    \"\"\"\n",
    "    Checks if a polygon is self-intersecting.\n",
    "\n",
    "    Args:\n",
    "        polygon_coordinates: A list of (x, y) tuples representing the polygon's vertices.\n",
    "\n",
    "    Returns:\n",
    "        True if the polygon is self-intersecting, False otherwise.\n",
    "    \"\"\"\n",
    "    polygon = Polygon(polygon_coordinates)\n",
    "    return not polygon.is_simple\n",
    "\n",
    "def grid_points_in_polygon(polygon_coords, spacing):\n",
    "    \"\"\"\n",
    "    Generates a grid of points within a polygon.\n",
    "\n",
    "    Args:\n",
    "        polygon_coords: A list of (x, y) tuples defining the polygon's vertices.\n",
    "        spacing: The desired spacing between grid points.\n",
    "\n",
    "    Returns:\n",
    "        A list of (x, y) tuples representing the grid points within the polygon.\n",
    "    \"\"\"\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "\n",
    "    x_coords = np.arange(min_x, max_x + spacing, spacing)\n",
    "    y_coords = np.arange(min_y, max_y + spacing, spacing)\n",
    "    xv, yv = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    grid_points_x = []\n",
    "    grid_points_y = []\n",
    "    for x, y in zip(xv.flatten(), yv.flatten()):\n",
    "        point = Point(x, y)\n",
    "        if polygon.contains(point):\n",
    "            grid_points_x.append(x)\n",
    "            grid_points_y.append(y)\n",
    "\n",
    "    grid_points = np.column_stack((grid_points_x, grid_points_y))\n",
    "\n",
    "    return grid_points\n",
    "\n",
    "def delaunay_triangulation_within_polygon(points, polygon):\n",
    "    \"\"\"\n",
    "    Generates a Delaunay triangulation of a set of points that lie within a given polygon.\n",
    "\n",
    "    Args:\n",
    "        points: A list of (x, y) coordinate tuples or a NumPy array of shape (n, 2) representing the points.\n",
    "        polygon: A {Link: Shapely Polygon https://shapely.readthedocs.io/en/stable/manual.html#polygon} object representing the polygon.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - A NumPy array of shape (n, 2) representing the coordinates of the vertices of the triangulation.\n",
    "        - A NumPy array of shape (m, 3) representing the indices of the vertices that form each triangle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Delaunay triangulation\n",
    "    delaunay = Delaunay(points)\n",
    "\n",
    "    # Filter triangles that fall outside the polygon\n",
    "    triangles_within_polygon = []\n",
    "    for triangle_indices in delaunay.simplices:\n",
    "        triangle_points = points[triangle_indices]\n",
    "        triangle_polygon = Polygon(triangle_points)\n",
    "\n",
    "        if triangle_polygon.within(polygon):\n",
    "            triangles_within_polygon.append(triangle_indices)\n",
    "\n",
    "    # Return the filtered triangles\n",
    "    return points, np.array(triangles_within_polygon)\n",
    "\n",
    "def shortest_distance_to_polygon(point_coords, polygon_coords):\n",
    "    \"\"\"\n",
    "    Calculates the shortest distance from a point to a polygon outline.\n",
    "\n",
    "    Args:\n",
    "        point_coords (tuple): Coordinates of the point (x, y).\n",
    "        polygon_coords (list): List of coordinate tuples defining the polygon's vertices.\n",
    "\n",
    "    Returns:\n",
    "        float: The shortest distance from the point to the polygon outline.\n",
    "    \"\"\"\n",
    "    point = Point(point_coords)\n",
    "    polygon = Polygon(polygon_coords)\n",
    "\n",
    "    # Get the boundary (outline) of the polygon as a LinearRing\n",
    "    polygon_boundary = polygon.boundary\n",
    "\n",
    "    # Find the nearest point on the polygon boundary to the given point\n",
    "    nearest_point = nearest_points(point, polygon_boundary)[1]\n",
    "\n",
    "    # Calculate the distance between the point and the nearest point on the boundary\n",
    "    distance = point.distance(nearest_point)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def lift_points_radially(points_3d, angle_degrees):\n",
    "    \"\"\"\n",
    "    Rotates each point around the axis perpendicular to its (x, y) vector,\n",
    "    lifting it into the Z direction by the given angle.\n",
    "    Points at the origin remain unchanged.\n",
    "\n",
    "    Parameters:\n",
    "        points_3d (np.ndarray): Nx3 array of points (x, y, z)\n",
    "        angle_degrees (float): Angle in degrees to lift into Z\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Nx3 array of rotated 3D points\n",
    "    \"\"\"\n",
    "    angle_radians = np.deg2rad(angle_degrees)\n",
    "    lifted_points = []\n",
    "\n",
    "    for x, y, z in points_3d:\n",
    "        if np.isclose(x, 0) and np.isclose(y, 0):\n",
    "            # Leave origin fixed\n",
    "            lifted_points.append([0, 0, z])\n",
    "            continue\n",
    "\n",
    "        # Get direction vector in XY plane from origin to point\n",
    "        v_xy = np.array([x, y, 0])\n",
    "        norm_xy = np.linalg.norm(v_xy)\n",
    "        v_xy_unit = v_xy / norm_xy\n",
    "\n",
    "        # Axis of rotation is perpendicular to (x, y, 0) in the XY plane\n",
    "        # Cross with Z to get rotation axis\n",
    "        axis = np.cross(v_xy_unit, [0, 0, 1])  # This is the rotation axis\n",
    "        axis = axis / np.linalg.norm(axis)\n",
    "\n",
    "        # Rodrigues' rotation formula\n",
    "        v = np.array([x, y, z])\n",
    "        k = axis\n",
    "        cos_theta = np.cos(angle_radians)\n",
    "        sin_theta = np.sin(angle_radians)\n",
    "\n",
    "        v_rot = (v * cos_theta +\n",
    "                 np.cross(k, v) * sin_theta +\n",
    "                 k * np.dot(k, v) * (1 - cos_theta))\n",
    "\n",
    "        lifted_points.append(v_rot)\n",
    "\n",
    "    return np.array(lifted_points)\n",
    "\n",
    "def triangle_area_2d(p0, p1, p2):\n",
    "    return 0.5 * np.abs((p1[0] - p0[0]) * (p2[1] - p0[1]) -\n",
    "                        (p2[0] - p0[0]) * (p1[1] - p0[1]))\n",
    "\n",
    "def triangle_area_3d(p0, p1, p2):\n",
    "    v1 = p1 - p0\n",
    "    v2 = p2 - p0\n",
    "    return 0.5 * np.linalg.norm(np.cross(v1, v2))\n",
    "\n",
    "def preserve_area(vertices_2d, faces, z_raw):\n",
    "    vertices_3d = np.hstack([vertices_2d, z_raw[:, np.newaxis]])\n",
    "    z_adjusted = z_raw.copy()\n",
    "\n",
    "    for tri in faces:\n",
    "        i0, i1, i2 = tri\n",
    "        p0, p1, p2 = vertices_2d[[i0, i1, i2]]\n",
    "        a_2d = triangle_area_2d(p0, p1, p2)\n",
    "\n",
    "        p0_3d = np.array([*p0, z_adjusted[i0]])\n",
    "        p1_3d = np.array([*p1, z_adjusted[i1]])\n",
    "        p2_3d = np.array([*p2, z_adjusted[i2]])\n",
    "        a_3d = triangle_area_3d(p0_3d, p1_3d, p2_3d)\n",
    "\n",
    "        if a_3d == 0 or a_2d == 0:\n",
    "            continue  # skip degenerate triangles\n",
    "\n",
    "        scale = np.sqrt(a_2d / a_3d)\n",
    "\n",
    "        # Scale the Z-values locally to match the area\n",
    "        centroid_z = (z_adjusted[i0] + z_adjusted[i1] + z_adjusted[i2]) / 3.0\n",
    "        for i in [i0, i1, i2]:\n",
    "            deviation = z_adjusted[i] - centroid_z\n",
    "            z_adjusted[i] = centroid_z + deviation * scale\n",
    "\n",
    "    return z_adjusted\n",
    "\n",
    "def get_parameters(parameter_file_name):\n",
    "\n",
    "    # read in parameter file\n",
    "    parameters = pd.read_csv(parameter_file_name)\n",
    "\n",
    "    # get parameter name\n",
    "    parameter_name = parameters[\"parameter_name\"].item()\n",
    "    \n",
    "    # morphospace and PC values to synthesize leaf shape\n",
    "    morphospace_file = parameters[\"morphospace_file\"].item()\n",
    "    PC_val_file = parameters[\"PC_val_file\"].item()\n",
    "    \n",
    "    # select two PC axes to sythesize leaves from\n",
    "    PCa = parameters[\"PCa\"].item() # index position of first PC\n",
    "    PCb = parameters[\"PCb\"].item() # index position of second PC\n",
    "    \n",
    "    # set density of grid to sample points within leaves\n",
    "    grid_density = parameters[\"grid_density\"].item() # density to sample points on leaves\n",
    "    \n",
    "    # select the range of leaf numbers to uniformly sample from\n",
    "    low_lf_num = parameters[\"low_lf_num\"].item() # inclusive\n",
    "    high_lf_num = parameters[\"high_lf_num\"].item() # up to, not including\n",
    "    \n",
    "    # set relative height of first leaf in series\n",
    "    low_juv_len = parameters[\"low_juv_len\"].item() # inclusive\n",
    "    high_juv_len = parameters[\"high_juv_len\"].item() # not including\n",
    "    \n",
    "    # set height of last leaf relative to one\n",
    "    low_adult_len = parameters[\"low_adult_len\"].item() # inclusive\n",
    "    high_adult_len = parameters[\"high_adult_len\"].item() # not including\n",
    "    \n",
    "    # set maximum scaling values for leaf curvature and leaf curling\n",
    "    lf_curve = parameters[\"lf_curve\"].item() # max scaling of leaf curvature (the interior of the leaf)\n",
    "    lf_curl = parameters[\"lf_curl\"].item() # max scaling of leaf curl (across length of leaf)\n",
    "    \n",
    "    # set maximum displacement at leaf tip (y value)\n",
    "    max_down_curl = parameters[\"max_down_curl\"].item()\n",
    "    max_up_curl = parameters[\"max_up_curl\"].item()\n",
    "    \n",
    "    # set minimum and maximum limits of leaf angles\n",
    "    ang_min_limit = parameters[\"ang_min_limit\"].item() # max possible min angle\n",
    "    ang_max_limit = parameters[\"ang_max_limit\"].item() # max possible angle\n",
    "    \n",
    "    # set phyllotaxy angle\n",
    "    phyllo_ang = parameters[\"phyllo_ang\"].item()\n",
    "\n",
    "    return parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang\n",
    "\n",
    "def generate_model_vals(parameter_file_name):\n",
    "    \n",
    "    # read in parameter values using get_parameters function\n",
    "    parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang = get_parameters(parameter_file_name)\n",
    "\n",
    "    # 0 = [-137.5077640500378546463487,137.5077640500378546463487]\n",
    "    # 1 = [-137.5077640500378546463487,137.5077640500378546463487,90,180]\n",
    "    # 2 = [-137.5077640500378546463487,137.5077640500378546463487,90,180,<random -360 to +360>]\n",
    "    # 3 = [<random -360 to +360>]\n",
    "\n",
    "    if phyllo_ang==0:\n",
    "        phyllo_ang=np.random.choice(np.array([-137.5077640500378546463487,137.5077640500378546463487]))\n",
    "    elif phyllo_ang==1:\n",
    "        phyllo_ang=np.random.choice(np.array([-137.5077640500378546463487,137.5077640500378546463487,90,180]))\n",
    "    elif phyllo_ang==2:\n",
    "        phyllo_ang=np.random.choice(np.array([-137.5077640500378546463487,137.5077640500378546463487,90,180,np.random.uniform(-360,360)]))\n",
    "    else:\n",
    "        phyllo_ang=np.random.uniform(-360,360)\n",
    "        \n",
    "    # select the number of leaves in the series\n",
    "    n = np.random.randint(low_lf_num, high_lf_num) \n",
    "    \n",
    "    # select relative height of first leaf in series\n",
    "    juv_len = np.random.uniform(low_juv_len, high_juv_len) \n",
    "    \n",
    "    # set node position of max leaf height of 1 (0 to 1)\n",
    "    node_max = np.random.uniform(0, 1) \n",
    "    \n",
    "    # set the relative \n",
    "    adult_len = np.random.uniform(low_adult_len, high_adult_len) \n",
    "    \n",
    "    # calculate the scaling factor for leaf curvature and leaf curl\n",
    "    leaf_curve = np.random.uniform(0,lf_curve) \n",
    "    leaf_curl = np.random.uniform(0,lf_curl) \n",
    "    \n",
    "    # calculate leaf curl amount\n",
    "    leaf_tip = np.random.uniform(max_down_curl,max_up_curl) \n",
    "    \n",
    "    # set middle of leaf curl (y value)\n",
    "    leaf_mid = np.random.uniform(0,leaf_tip) \n",
    "    \n",
    "    # calculate elevation angles\n",
    "    angle_min = np.random.uniform(0,ang_min_limit) # get the minimum angle\n",
    "    angle_max = np.random.uniform(angle_min,ang_max_limit) # get the maximum angle\n",
    "    \n",
    "    # find the two points in PCA space to make the leaf series\n",
    "    # Load PCA model\n",
    "    with open(morphospace_file, 'rb') as file:\n",
    "        pca = pickle.load(file)\n",
    "    # Load in PC values\n",
    "    PCs = np.load(PC_val_file)\n",
    "    \n",
    "    # retrieve just the desired two PCs\n",
    "    just_PCs = PCs[:,[PCa,PCb]]\n",
    "    # create the convex hull\n",
    "    hull = ConvexHull(just_PCs)\n",
    "    # retreieve hull points\n",
    "    hull_points = just_PCs[hull.vertices]\n",
    "    # find two random points in 2D convex hull\n",
    "    # these are the terminal points of the line\n",
    "    term_pts = dist_in_hull(hull_points,2)\n",
    "    # these are the points\n",
    "    start_x = term_pts[0][0]\n",
    "    start_y = term_pts[0][1]\n",
    "    end_x = term_pts[1][0]\n",
    "    end_y = term_pts[1][0]\n",
    "\n",
    "    return parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y,end_x,end_y\n",
    "\n",
    "def get_model_val_df(parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y,end_x,end_y):\n",
    "    \n",
    "    # create pandas dataframe to save files\n",
    "    model_val_file = pd.DataFrame({\n",
    "    \"parameter_name\":[parameter_name],\n",
    "    \"morphospace_file\":[morphospace_file],\n",
    "    \"PC_val_file\":[PC_val_file],\n",
    "    \"PCa\":[PCa], \n",
    "    \"PCb\":[PCb], \n",
    "    \"grid_density\":[grid_density], \n",
    "    \"low_lf_num\":[low_lf_num], \n",
    "    \"high_lf_num\":[high_lf_num], \n",
    "    \"low_juv_len\":[low_juv_len], \n",
    "    \"high_juv_len\":[high_juv_len], \n",
    "    \"low_adult_len\":[low_adult_len], \n",
    "    \"high_adult_len\":[high_adult_len], \n",
    "    \"lf_curve\":[lf_curve], \n",
    "    \"lf_curl\":[lf_curl], \n",
    "    \"max_down_curl\":[max_down_curl], \n",
    "    \"max_up_curl\":[max_up_curl], \n",
    "    \"ang_min_limit\":[ang_min_limit], \n",
    "    \"ang_max_limit\":[ang_max_limit], \n",
    "    \"phyllo_ang\":[phyllo_ang],\n",
    "    \"n\":[n],\n",
    "    \"juv_len\":[juv_len],\n",
    "    \"node_max\":[node_max],\n",
    "    \"adult_len\":[adult_len],\n",
    "    \"leaf_curve\":[leaf_curve],\n",
    "    \"leaf_curl\":[leaf_curl],\n",
    "    \"leaf_tip\":[leaf_tip],\n",
    "    \"leaf_mid\":[leaf_mid],\n",
    "    \"angle_min\":[angle_min],\n",
    "    \"angle_max\":[angle_max],\n",
    "    \"start_x\":[start_x], \n",
    "    \"start_y\":[start_y], \n",
    "    \"end_x\":[end_x],\n",
    "    \"end_y\":[end_y],\n",
    "    })\n",
    "\n",
    "    # return the model value df\n",
    "    return model_val_file\n",
    "\n",
    "def get_model_vals(model_val_file_name):\n",
    "\n",
    "    # read in parameter file\n",
    "    model_vals = pd.read_csv(model_val_file_name)\n",
    "\n",
    "    # get values\n",
    "    parameter_name=model_vals[\"parameter_name\"].item()\n",
    "    morphospace_file=model_vals[\"morphospace_file\"].item() \n",
    "    PC_val_file=model_vals[\"PC_val_file\"].item() \n",
    "    PCa=model_vals[\"PCa\"].item() \n",
    "    PCb=model_vals[\"PCb\"].item() \n",
    "    grid_density=model_vals[\"grid_density\"].item() \n",
    "    low_lf_num=model_vals[\"low_lf_num\"].item() \n",
    "    high_lf_num=model_vals[\"high_lf_num\"].item() \n",
    "    low_juv_len=model_vals[\"low_juv_len\"].item() \n",
    "    high_juv_len=model_vals[\"high_juv_len\"].item() \n",
    "    low_adult_len=model_vals[\"low_adult_len\"].item() \n",
    "    high_adult_len=model_vals[\"high_adult_len\"].item() \n",
    "    lf_curve=model_vals[\"lf_curve\"].item() \n",
    "    lf_curl=model_vals[\"lf_curl\"].item() \n",
    "    max_down_curl=model_vals[\"max_down_curl\"].item() \n",
    "    max_up_curl=model_vals[\"max_up_curl\"].item()\n",
    "    ang_min_limit=model_vals[\"ang_min_limit\"].item() \n",
    "    ang_max_limit=model_vals[\"ang_max_limit\"].item() \n",
    "    phyllo_ang=model_vals[\"phyllo_ang\"].item()\n",
    "    n=model_vals[\"n\"].item()\n",
    "    juv_len=model_vals[\"juv_len\"].item()\n",
    "    node_max=model_vals[\"node_max\"].item()\n",
    "    adult_len=model_vals[\"adult_len\"].item()\n",
    "    leaf_curve=model_vals[\"leaf_curve\"].item()\n",
    "    leaf_curl=model_vals[\"leaf_curl\"].item()\n",
    "    leaf_tip=model_vals[\"leaf_tip\"].item()\n",
    "    leaf_mid=model_vals[\"leaf_mid\"].item()\n",
    "    angle_min=model_vals[\"angle_min\"].item()\n",
    "    angle_max=model_vals[\"angle_max\"].item()\n",
    "    start_x=model_vals[\"start_x\"].item()\n",
    "    start_y=model_vals[\"start_y\"].item()\n",
    "    end_x=model_vals[\"end_x\"].item()\n",
    "    end_y=model_vals[\"end_y\"].item()\n",
    "    \n",
    "    return parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y\n",
    "\n",
    "def fibonacci_sphere(samples=1000):\n",
    "    points = []\n",
    "    phi = math.pi * (math.sqrt(5.) - 1.)  # golden angle in radians\n",
    "    for i in range(samples):\n",
    "        y = 1 - (i / float(samples - 1)) * 2  # y goes from 1 to -1\n",
    "        radius = math.sqrt(1 - y * y)  # radius at y\n",
    "        theta = phi * i  # golden angle increment\n",
    "        x = math.cos(theta) * radius\n",
    "        z = math.sin(theta) * radius\n",
    "        points.append((x, y, z))\n",
    "    return points\n",
    "\n",
    "def ray_casting(n_origins,radius,n_sample,v,parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y):\n",
    "\n",
    "    #####\n",
    "    # READ BACK IN PCA INFORMATION\n",
    "    #####\n",
    "    pca = pickle.load(open(morphospace_file,'rb')) \n",
    "    PCs = np.load(PC_val_file)\n",
    "    \n",
    "    #####\n",
    "    # SPECIFY NUMBER OF LEAVES IN LEAF SERIES\n",
    "    #####\n",
    "    \n",
    "    # Create an array of evenly spaced values from 0 to 1\n",
    "    t_values = np.linspace(0, 1, n)\n",
    "    \n",
    "    start_point = np.array([start_x, start_y])\n",
    "    end_point = np.array([end_x, end_y])\n",
    "    \n",
    "    # Calculate the points using linear interpolation\n",
    "    points = np.array([start_point + t * (end_point - start_point) for t in t_values])\n",
    "    \n",
    "    # calculate eigen leaves\n",
    "    eigen_arr = np.zeros((len(points),np.shape(PCs)[1])) # create an array for eigenleaf, length of number of PCs\n",
    "    eigen_arr[:,PCa] = points[:,0] # set the PCa values\n",
    "    eigen_arr[:,PCb] = points[:,1] # set the PCb values\n",
    "    \n",
    "    # calculate the inverse eigenleaf\n",
    "    inv_leaf = pca.inverse_transform(eigen_arr)\n",
    "    inv_x = inv_leaf[:,0::2] # select just inverse x vals\n",
    "    inv_y = inv_leaf[:,1::2] # select just inverse y vals\n",
    "    \n",
    "    # create an array of the leaf series\n",
    "    leaf_lines = np.stack((inv_x, inv_y), axis=1)\n",
    "    leaf_lines = np.swapaxes(leaf_lines, 1,2)\n",
    "    \n",
    "    #####\n",
    "    # CHECK FOR SELF-INTERSECTION\n",
    "    #####\n",
    "    \n",
    "    self_intersect = 0 # assume no self intersect\n",
    "    for i in range(len(leaf_lines)):\n",
    "        if is_self_intersecting(leaf_lines[i]):\n",
    "            self_intersect+=1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #####\n",
    "    # TRANSLATE LEAF BASE TO ORIGIN\n",
    "    #####\n",
    "    \n",
    "    trans_lfs = np.zeros(np.shape(leaf_lines))\n",
    "    \n",
    "    for i in range(len(leaf_lines)):\n",
    "        start_pt = leaf_lines[i,0,:] # leaf start\n",
    "        end_pt = leaf_lines[i,-1,:] # leaf end\n",
    "        base_pt = np.mean((start_pt, end_pt), axis=0) # base point of leaf\n",
    "        trans_lf = leaf_lines[i,:,:] - base_pt # translated lf\n",
    "        trans_lfs[i,:,:] = trans_lf # save translated lf\n",
    "    \n",
    "    #####\n",
    "    # SCALE LEAF LENGTHS ACROSS LEAF SERIES\n",
    "    #####\n",
    "    \n",
    "    # find relative lengths for each leaf (to 1)\n",
    "    coeffs = np.polyfit(x=[0,node_max,1], y=[juv_len,1,adult_len], deg=2) # calculate coefficients\n",
    "    func = np.poly1d(coeffs) # calculate 2D polynomial function\n",
    "    lf_lens = func(np.linspace(0,1,n)) # calculate leaf lengths relative to 1\n",
    "    \n",
    "    scaled_lfs = np.zeros(np.shape(trans_lfs))\n",
    "    for i in range(n): # for each leaf in the series\n",
    "        scale_len_lf = trans_lfs[i,:,:]/np.max(trans_lfs[i,:,1]) # scale leaf len to 1\n",
    "        scale_len_lf = scale_len_lf*lf_lens[i] # scale leaf to desired length\n",
    "        scaled_lfs[i,:,:] = scale_len_lf # save scaled length leaf\n",
    "    \n",
    "    \n",
    "    #####\n",
    "    # CREATE A LEAF SERIES FOR VALIDATION, ALL WITH V NUMBER LEAVES IN SERIES\n",
    "    #####\n",
    "    \n",
    "    v_values = np.linspace(0, 1, v) # v is for validating leaf shapes\n",
    "    v_points = np.array([start_point + v * (end_point - start_point) for v in v_values])\n",
    "    valid_arr = np.zeros((len(v_points),np.shape(PCs)[1]))\n",
    "    valid_arr[:,PCa] = v_points[:,0] # set the PCa values\n",
    "    valid_arr[:,PCb] = v_points[:,1] # set the PCb values\n",
    "    \n",
    "    val_leaf = pca.inverse_transform(valid_arr)\n",
    "    val_x = val_leaf[:,0::2] \n",
    "    val_y = val_leaf[:,1::2] \n",
    "    \n",
    "    val_lines = np.stack((val_x, val_y), axis=1)\n",
    "    val_lines = np.swapaxes(val_lines, 1,2)\n",
    "    \n",
    "    vtrans_lfs = np.zeros(np.shape(val_lines))\n",
    "    for i in range(len(val_lines)):\n",
    "        start_pt = val_lines[i,0,:] # leaf start\n",
    "        end_pt = val_lines[i,-1,:] # leaf end\n",
    "        base_pt = np.mean((start_pt, end_pt), axis=0) # base point of leaf\n",
    "        vtrans_lf = val_lines[i,:,:] - base_pt # translated lf\n",
    "        vtrans_lfs[i,:,:] = vtrans_lf # save translated lf\n",
    "    \n",
    "    # find relative lengths for each leaf (to 1)\n",
    "    coeffs = np.polyfit(x=[0,node_max,1], y=[juv_len,1,adult_len], deg=2) # calculate coefficients\n",
    "    func = np.poly1d(coeffs) # calculate 2D polynomial function\n",
    "    vlf_lens = func(np.linspace(0,1,v)) # calculate leaf lengths relative to 1\n",
    "    \n",
    "    vscaled_lfs = np.zeros(np.shape(vtrans_lfs))\n",
    "    for i in range(v): # for each leaf in the series\n",
    "        vscale_len_lf = vtrans_lfs[i,:,:]/np.max(vtrans_lfs[i,:,1]) # scale leaf len to 1\n",
    "        vscale_len_lf = vscale_len_lf*vlf_lens[i] # scale leaf to desired length\n",
    "        vscaled_lfs[i,:,:] = vscale_len_lf # save scaled length leaf\n",
    "    \n",
    "    #####\n",
    "    # CALCULATE LEAF SURFACE VALUES AND ELEVATION\n",
    "    #####\n",
    "    \n",
    "    pts_3D = [] # list of x, y, z \n",
    "    tris_3D = [] # list of triangles\n",
    "    \n",
    "    for i in range(len(scaled_lfs)): # for each leaf\n",
    "    \n",
    "        curr_lf = scaled_lfs[i] # get current leaf\n",
    "        \n",
    "        grid_pts = grid_points_in_polygon(curr_lf, grid_density) # calculate grid points\n",
    "        \n",
    "        # Delauney triangulation\n",
    "        # tris = indices of individual triangles in points\n",
    "        pts, tris = delaunay_triangulation_within_polygon(grid_pts, Polygon(curr_lf)) # calculate Delauney triangulation\n",
    "        \n",
    "        # get distances of each grid point to polygon boundary\n",
    "        dists=[]\n",
    "        for j in range(len(pts)):\n",
    "            dists.append(shortest_distance_to_polygon((pts[j,0],pts[j,1]),curr_lf))\n",
    "        scaled_lf_curve = np.array(dists)*leaf_curve # get scaled leaf curvature\n",
    "        \n",
    "        # calculate leaf curl from distance to base\n",
    "        coeffs = np.polyfit(x=[0,0.5,1], y=[0,leaf_mid,leaf_tip], deg=2) # calculate coefficients\n",
    "        func = np.poly1d(coeffs) # calculate 2D polynomial function\n",
    "        \n",
    "        lf_curl_vals = func(pts[:,1])*leaf_curl*np.max(pts[:,1]) # calculate leaf lengths relative to 1\n",
    "        \n",
    "        lf_surf_vals = scaled_lf_curve+lf_curl_vals # calculate overall curvature\n",
    "    \n",
    "        lf_surf_lf = np.column_stack((pts, lf_surf_vals)) # combine x, y, and z vals\n",
    "    \n",
    "        # translate the z axis to the origin\n",
    "        lf_surf_lf[:,2] = lf_surf_lf[:,2] - np.mean(lf_surf_lf[:,2][lf_surf_lf[:,1]==np.min(lf_surf_lf[:,1])]  )\n",
    "    \n",
    "        # rotate leaf\n",
    "        rot_x, rot_y = rotate_points(lf_surf_lf[:,0],lf_surf_lf[:,1], phyllo_ang*i)\n",
    "        lf_surf_lf[:,0:2] = np.column_stack((rot_x, rot_y))\n",
    "    \n",
    "        # Warp Dlauney triangulation into 3D\n",
    "        # get surface values of leaf\n",
    "        z_raw = lf_surf_lf[:,2]\n",
    "    \n",
    "        # get the x and y coordinate values\n",
    "        vertices_2d = lf_surf_lf[:,0:2]\n",
    "        \n",
    "        # Area-preserving adjustment\n",
    "        z_warped = preserve_area(vertices_2d, tris, z_raw)\n",
    "        \n",
    "        # Now vertices_3d = [x, y, z_warped]\n",
    "        vertices_3d = np.hstack([vertices_2d, z_warped[:, np.newaxis]])\n",
    "    \n",
    "        # Elevate the leaves\n",
    "        # elevate the points\n",
    "        angs = np.linspace(angle_min, angle_max, n)\n",
    "        elv_pts = lift_points_radially(vertices_3d, angs[i])\n",
    "        \n",
    "        pts_3D.append(elv_pts) # append points to list\n",
    "        tris_3D.append(tris) # append triangle \n",
    "    \n",
    "    #####\n",
    "    # NORMALIZE RADIUS TO FARTHEST POINT FROM ORIGIN\n",
    "    #####\n",
    "    \n",
    "    dists = []\n",
    "    for i in range(len(pts_3D)):\n",
    "        for j in range(len(pts_3D[i])):\n",
    "            dists.append(np.sqrt( ((pts_3D[i][j][0])**2 +\n",
    "                                   (pts_3D[i][j][1])**2 +\n",
    "                                   (pts_3D[i][j][2])**2)\n",
    "                                ))\n",
    "    \n",
    "    max_dist = np.max(dists) # get the max radius to normalize by\n",
    "    \n",
    "    norm_pts_3D = [] # normalize 3D points\n",
    "    for i in range(len(pts_3D)):\n",
    "        norm_pts_3D.append(pts_3D[i]/max_dist)\n",
    "    \n",
    "    pts_3D = norm_pts_3D\n",
    "    \n",
    "    #####\n",
    "    # CREATE MESH\n",
    "    #####\n",
    "    \n",
    "    # create mesh of first leaf\n",
    "    mesh = trimesh.Trimesh(vertices=pts_3D[0], faces=tris_3D[0])\n",
    "    \n",
    "    # concatenate the meshes of other leaves\n",
    "    for i in range(1, len(pts_3D)):\n",
    "        new_mesh = trimesh.Trimesh(vertices=pts_3D[i], faces=tris_3D[i])\n",
    "        mesh = trimesh.util.concatenate(mesh, new_mesh)\n",
    "    \n",
    "    # Get the vertices (coordinates of each vertex)\n",
    "    vertices = mesh.vertices\n",
    "    \n",
    "    # Get the faces (indices of vertices that form each triangle)\n",
    "    faces = mesh.faces\n",
    "    \n",
    "    #####\n",
    "    # CALCULATE RAY ORIGINS\n",
    "    #####\n",
    "    \n",
    "    fibo_pts = fibonacci_sphere(samples = n_origins) # calculate points\n",
    "    fibo_arr = np.array(fibo_pts) # turn tuples into array\n",
    "    ray_origins = fibo_arr[fibo_arr[:,2]>0]*radius # select points in the positive hemisphere\n",
    "    \n",
    "    #####\n",
    "    # PERFORM RAY CASTING\n",
    "    #####\n",
    "    \n",
    "    # --- Get face centers of mesh ---\n",
    "    face_centers = mesh.triangles_center  # shape (M, 3)\n",
    "    n_faces = len(face_centers)\n",
    "    n_origins = len(ray_origins)\n",
    "    \n",
    "    # --- Build ray origins and directions ---\n",
    "    ray_origins_batch = []\n",
    "    ray_directions_batch = []\n",
    "    intended_face_indices = []\n",
    "    \n",
    "    for origin_idx, origin in enumerate(ray_origins):\n",
    "        dirs = face_centers - origin  # (n_faces, 3)\n",
    "        dirs /= np.linalg.norm(dirs, axis=1, keepdims=True)\n",
    "        \n",
    "        ray_origins_batch.append(np.repeat(origin[np.newaxis, :], n_faces, axis=0))\n",
    "        ray_directions_batch.append(dirs)\n",
    "        intended_face_indices.append(np.arange(n_faces))  # face indices\n",
    "    \n",
    "    # Flatten arrays\n",
    "    ray_origins_batch = np.vstack(ray_origins_batch)  # shape (N, 3)\n",
    "    ray_directions_batch = np.vstack(ray_directions_batch)  # shape (N, 3)\n",
    "    intended_face_indices = np.tile(np.arange(n_faces), n_origins)  # shape (N,)\n",
    "    \n",
    "    # --- Subsample rays ---\n",
    "    N_total = len(ray_origins_batch)\n",
    "    n_sample = min(n_sample, N_total)\n",
    "    sample_idx = np.random.choice(N_total, size=n_sample, replace=False)\n",
    "    \n",
    "    ray_origins_sub = ray_origins_batch[sample_idx]\n",
    "    ray_directions_sub = ray_directions_batch[sample_idx]\n",
    "    intended_faces_sub = intended_face_indices[sample_idx]\n",
    "    \n",
    "    # --- Cast rays ---\n",
    "    locations, ray_index, tri_index = mesh.ray.intersects_location(\n",
    "        ray_origins_sub, \n",
    "        ray_directions_sub, \n",
    "        multiple_hits=False\n",
    "    )\n",
    "    \n",
    "    # --- Determine interception correctness ---\n",
    "    hit_intended = tri_index == intended_faces_sub[ray_index]\n",
    "    proportion_correct = np.sum(hit_intended) / len(ray_index)\n",
    "    proportion_incorrect = 1 - proportion_correct\n",
    "    total_area = mesh.area\n",
    "\n",
    "    return proportion_correct, proportion_incorrect, self_intersect, total_area, vscaled_lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df500f7c-246b-42f0-a464-b18774128efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x256_nahuatl_parameters_may_11_2025.csv\n",
      "x626_nahuatl_parameters_may_11_2025.csv\n",
      "x19_nahuatl_parameters_may_11_2025.csv\n",
      "x415_nahuatl_parameters_may_11_2025.csv\n",
      "x590_nahuatl_parameters_may_11_2025.csv\n",
      "x848_nahuatl_parameters_may_11_2025.csv\n",
      "x900_nahuatl_parameters_may_11_2025.csv\n",
      "x885_nahuatl_parameters_may_11_2025.csv\n",
      "x681_nahuatl_parameters_may_11_2025.csv\n",
      "x73_nahuatl_parameters_may_11_2025.csv\n",
      "x704_nahuatl_parameters_may_11_2025.csv\n",
      "x374_nahuatl_parameters_may_11_2025.csv\n",
      "x822_nahuatl_parameters_may_11_2025.csv\n",
      "x147_nahuatl_parameters_may_11_2025.csv\n",
      "x537_nahuatl_parameters_may_11_2025.csv\n",
      "x315_nahuatl_parameters_may_11_2025.csv\n",
      "x290_nahuatl_parameters_may_11_2025.csv\n",
      "x12_nahuatl_parameters_may_11_2025.csv\n",
      "x765_nahuatl_parameters_may_11_2025.csv\n",
      "x843_nahuatl_parameters_may_11_2025.csv\n",
      "x556_nahuatl_parameters_may_11_2025.csv\n",
      "x126_nahuatl_parameters_may_11_2025.csv\n",
      "x647_nahuatl_parameters_may_11_2025.csv\n",
      "x237_nahuatl_parameters_may_11_2025.csv\n",
      "x78_nahuatl_parameters_may_11_2025.csv\n",
      "x181_nahuatl_parameters_may_11_2025.csv\n",
      "x829_nahuatl_parameters_may_11_2025.csv\n",
      "x474_nahuatl_parameters_may_11_2025.csv\n",
      "x961_nahuatl_parameters_may_11_2025.csv\n",
      "x716_nahuatl_parameters_may_11_2025.csv\n",
      "x693_nahuatl_parameters_may_11_2025.csv\n",
      "x61_nahuatl_parameters_may_11_2025.csv\n",
      "x366_nahuatl_parameters_may_11_2025.csv\n",
      "x155_nahuatl_parameters_may_11_2025.csv\n",
      "x978_nahuatl_parameters_may_11_2025.csv\n",
      "x525_nahuatl_parameters_may_11_2025.csv\n",
      "x198_nahuatl_parameters_may_11_2025.csv\n",
      "x830_nahuatl_parameters_may_11_2025.csv\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "# GENERATE PARAMETER FILE\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "# save parameter name (without extension)\n",
    "parameter_name = \"nahuatl_parameters_may_11_2025\"\n",
    "\n",
    "parameter_file = pd.DataFrame({\n",
    "\n",
    "# save parameter name\n",
    "\"parameter_name\":parameter_name,\n",
    "\n",
    "# morphospace and PC values to synthesize leaf shape\n",
    "\"morphospace_file\":[\"nahuatl_morphospace_may_10_2025.pkl\"], # PCA model\n",
    "\"PC_val_file\":[\"nahuatl_PCs_may_10_2025.npy\"], # empirical PCA values from the PCA\n",
    "\n",
    "# select two PC axes to sythesize leaves from\n",
    "\"PCa\":[0], # index position of first PC (index 0 = PC1)\n",
    "\"PCb\":[4], # index position of second PC (index 0 = PC1)\n",
    "\n",
    "# set density of grid to sample points within leaves\n",
    "\"grid_density\":[0.01], # density to sample points on leaves\n",
    "\n",
    "# select the range of leaf numbers to uniformly sample from\n",
    "\"low_lf_num\":[3], # inclusive\n",
    "\"high_lf_num\":[10], # up to, not including\n",
    "\n",
    "# set relative height of first leaf in series\n",
    "\"low_juv_len\":[0.2], # inclusive\n",
    "\"high_juv_len\":[1], # not including\n",
    "\n",
    "# set height of last leaf relative to one\n",
    "\"low_adult_len\":[0.2], # inclusive\n",
    "\"high_adult_len\":[1], # not including\n",
    "\n",
    "# set maximum scaling values for leaf curvature and leaf curling\n",
    "\"lf_curve\":[0.8], # max scaling of leaf curvature (the interior of the leaf)\n",
    "\"lf_curl\":[0.8], # max scaling of leaf curl (across length of leaf)\n",
    "\n",
    "# set maximum displacement at leaf tip (y value)\n",
    "\"max_down_curl\":[-1],\n",
    "\"max_up_curl\":[1],\n",
    "\n",
    "# set minimum and maximum limits of leaf angles\n",
    "\"ang_min_limit\":[10], # max possible min angle\n",
    "\"ang_max_limit\":[90], # max possible angle\n",
    "\n",
    "# set phyllotaxy angle (in degrees)\n",
    "# select from options below:\n",
    "# 0 = [-137.5077640500378546463487,137.5077640500378546463487]\n",
    "# 1 = [-137.5077640500378546463487,137.5077640500378546463487,90,180]\n",
    "# 2 = [-137.5077640500378546463487,137.5077640500378546463487,90,180,<random -360 to +360>]\n",
    "# 3 = [<random -360 to +360>]\n",
    "\"phyllo_ang\":[2]\n",
    "\n",
    "})\n",
    "\n",
    "# save parameters to file\n",
    "parameter_file.to_csv(parameter_name+\".csv\", index=False)\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "# CREATE MORPHOLOGICAL MODELS\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "# double check parameter name (no file extension) \n",
    "parameter_name = \"nahuatl_parameters_may_11_2025\"\n",
    "\n",
    "# specify the number of desired models\n",
    "model_num = 1000\n",
    "\n",
    "# create folder to save model values\n",
    "destination_folder_path = \"model_vals\" \n",
    "\n",
    "if not os.path.exists(destination_folder_path): # check if the folder exists\n",
    "    # create the folder if it doesn't exist\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for i in range(model_num):\n",
    "\n",
    "    # GENERATE MODEL VALUES\n",
    "    # FUNCTION: generate_model_vals()\n",
    "    parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y = generate_model_vals(parameter_name+\".csv\")\n",
    "    \n",
    "    # CREATE MODEL VAL DATAFRAME\n",
    "    # FUNCTION: get_model_val_df()\n",
    "    model_val_df = get_model_val_df(parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y)\n",
    "\n",
    "    # SAVE MODEL VALUES\n",
    "    # file names = \"x\" + i + parameter space name\n",
    "    model_val_df.to_csv('./'+destination_folder_path+\"/x\"+str(i)+\"_\"+parameter_name+\".csv\", index=False) \n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "# RAY CASTING\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "####################\n",
    "# FETCH MODEL VALUES\n",
    "####################\n",
    "directory_path = \"./model_vals\" # select path of model values\n",
    "file_names = os.listdir(directory_path) # get file names\n",
    "\n",
    "for i in range(len(file_names)): # make sure no hidden files\n",
    "    if file_names[i][0]==\".\":\n",
    "        file_names.remove(i)\n",
    "        \n",
    "####################\n",
    "# CREATE FOLDER TO SAVE RESULTS\n",
    "####################\n",
    "# create folder to save ray casting results\n",
    "results_path = \"ray_casting_results\" \n",
    "\n",
    "# create folder to save leaf shape validation\n",
    "shape_path = \"leaf_shape_results\" \n",
    "\n",
    "if not os.path.exists(results_path): # check if the folder exists\n",
    "    # create the folder if it doesn't exist\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "if not os.path.exists(shape_path): # check if the folder exists\n",
    "    # create the folder if it doesn't exist\n",
    "    os.makedirs(shape_path)\n",
    "    \n",
    "####################\n",
    "# RETRIEVE MODEL AND PERFORM RAY CASTING\n",
    "####################\n",
    "\n",
    "problem_models = []\n",
    "for file in file_names: # for each model\n",
    "    \n",
    "    print(file) # print file name\n",
    "\n",
    "    # GET MODEL VALUES FOR THE CURRENT MODEL\n",
    "    # FUNCTION: get_model_vals()\n",
    "    parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y = get_model_vals(model_val_file_name=directory_path+\"/\"+file)\n",
    "    \n",
    "    # PARAMETERS FOR RAY CASTING\n",
    "    n_origins = 1000 # set number of sphere points, ray origins\n",
    "    radius = 2 # set radius of hemisphere of ray origins\n",
    "    \n",
    "    # SET HOW MANY RAYS TO SAMPLE\n",
    "    n_sample = 1000 \n",
    "    \n",
    "    # SET HOW MANY LEAF SHAPES IN SERIES TO VALIDATE\n",
    "    v = 5\n",
    "    \n",
    "    # PERFORM RAY CASTING FOR MODELS\n",
    "    # FUNCTION: ray_casting()\n",
    "    try:\n",
    "        proportion_correct, proportion_incorrect, self_intersect, total_area, vscaled_lfs = ray_casting(n_origins,radius,n_sample,v,parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y)\n",
    "        # SAVE RESULTS\n",
    "        np.save(results_path+'/results_'+file[:-4]+\".npy\", np.array([proportion_correct, proportion_incorrect, self_intersect, total_area]))\n",
    "        np.save(shape_path+'/shapes_'+file[:-4]+\".npy\", vscaled_lfs)\n",
    "    except Exception as e:\n",
    "        problem_models.append(file)\n",
    "        np.savetxt(\"problem_models.txt\", np.array(problem_models, dtype=str), fmt=\"%s\", newline='\\n')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e526d-da36-4b12-a894-e22248038025",
   "metadata": {},
   "source": [
    "# Problem files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d84f02c-3beb-408b-af6c-9474782eca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_files = np.loadtxt(\"./problem_models.txt\", delimiter=\",\", dtype=str)\n",
    "\n",
    "i=0\n",
    "file = prob_files[i]\n",
    "\n",
    "model_val_file = \"./model_vals/\"+file\n",
    "\n",
    "# GET MODEL VALUES FOR THE CURRENT MODEL\n",
    "# FUNCTION: get_model_vals()\n",
    "parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y = get_model_vals(model_val_file_name=\"./model_vals/\"+file)\n",
    "\n",
    "# PARAMETERS FOR RAY CASTING\n",
    "n_origins = 1000 # set number of sphere points, ray origins\n",
    "radius = 2 # set radius of hemisphere of ray origins\n",
    "\n",
    "# SET HOW MANY RAYS TO SAMPLE\n",
    "n_sample = 1000 \n",
    "\n",
    "# SET HOW MANY LEAF SHAPES IN SERIES TO VALIDATE\n",
    "v = 5\n",
    "\n",
    "# PERFORM RAY-CASTING, SEE ERROR\n",
    "proportion_correct, proportion_incorrect, self_intersect, total_area, vscaled_lfs = ray_casting(n_origins,radius,n_sample,v,parameter_name, morphospace_file, PC_val_file, PCa, PCb, grid_density, low_lf_num, high_lf_num, low_juv_len, high_juv_len, low_adult_len, high_adult_len, lf_curve, lf_curl, max_down_curl, max_up_curl, ang_min_limit, ang_max_limit, phyllo_ang,n,juv_len,node_max,adult_len,leaf_curve,leaf_curl,leaf_tip,leaf_mid,angle_min,angle_max,start_x,start_y, end_x,end_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ecc36-a08c-488e-8ac2-99b62f802f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
